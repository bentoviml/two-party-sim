{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c07e085",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "This notebook explores a further question: if we hone in to the Forward Looking Proposer and Probabilistic Responder, can we find an equilibrium approach? What if we complicate these strategies further and make them level-k aware of their opponents strategies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c359f1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize_scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be5df36",
   "metadata": {},
   "source": [
    "### Player 2\n",
    "---\n",
    "This code shows how player 2 would optimize their $alpha$ value, i.e. optimizing how high or low variance they should be about their utility to maximize their utility (minimize their loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b68a7813",
   "metadata": {},
   "outputs": [],
   "source": [
    "_x = None\n",
    "_u_bad_2 = None\n",
    "\n",
    "# Responder Player (P2: Democrat)\n",
    "def p_accept(x, alpha, u_bad_2):\n",
    "    utility_diff = -x - u_bad_2\n",
    "    return 1 / (1 + np.exp(-alpha * utility_diff))\n",
    "\n",
    "def utility_p2(x, alpha, u_bad_2):\n",
    "    p = p_accept(x, alpha, u_bad_2)\n",
    "    return -x * p + u_bad_2 * (1 - p)\n",
    "\n",
    "def neg_utility_p2(alpha):\n",
    "    # representing that p2 gets the inverse of the offer provided\n",
    "    return -utility_p2(_x, alpha, _u_bad_2)\n",
    "\n",
    "def optimize_alpha(x, u_bad_2):\n",
    "    global _x, _u_bad_2\n",
    "    _x = x\n",
    "    _u_bad_2 = u_bad_2\n",
    "    result = minimize_scalar(neg_utility_p2, bounds=(0.01, 10), method='bounded')\n",
    "    return result.x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e08934a",
   "metadata": {},
   "source": [
    "### Player 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e77e035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_utility_p1_fixed(x, alpha, depth, u_bad_1, u_resp, p, u_bad_2):\n",
    "    \"\"\"\n",
    "    This is for the basic case where p is fixed\n",
    "    \"\"\"\n",
    "    p_acc = p_accept(x, alpha, u_bad_2)\n",
    "\n",
    "    if depth == 0:\n",
    "        return x * p_acc + u_bad_1 * (1 - p_acc)\n",
    "\n",
    "    continuation = (1 - p) * expected_utility_p1_fixed(x, alpha, depth - 1, u_bad_1, u_resp, p, u_bad_2) + p * u_resp\n",
    "\n",
    "    utility_if_accepted = x + continuation\n",
    "    utility_if_rejected = u_bad_1 + continuation\n",
    "\n",
    "    return p_acc * utility_if_accepted + (1 - p_acc) * utility_if_rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9265635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_alpha = None\n",
    "_u_bad_1 = None\n",
    "_u_resp = None\n",
    "_p = None\n",
    "_u_bad_2 = None\n",
    "_depth = None\n",
    "_p_bump = None\n",
    "\n",
    "def neg_expected_utility_p1(x):\n",
    "    return -expected_utility_p1_fixed(x, _alpha, _depth, _u_bad_1, _u_resp, _p, _u_bad_2)\n",
    "\n",
    "def optimize_x(alpha, depth, u_bad_1, u_resp, p, u_bad_2):\n",
    "    global _alpha, _depth, _u_bad_1, _u_resp, _p, _u_bad_2\n",
    "    _alpha = alpha\n",
    "    _depth = depth\n",
    "    _u_bad_1 = u_bad_1\n",
    "    _u_resp = u_resp\n",
    "    _p = p\n",
    "    _u_bad_2 = u_bad_2\n",
    "    \n",
    "    result = minimize_scalar(neg_expected_utility_p1, bounds=(0, 100), method='bounded')\n",
    "    return result.x, -result.fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77d840c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_utility_p1_dynamic(x, alpha, depth, u_bad_1, u_resp, p, p_bump, u_bad_2):\n",
    "    \"\"\"\n",
    "    This is for the case where p is impacted by rejection\n",
    "    \"\"\"\n",
    "    p_acc = p_accept(x, alpha, u_bad_2)\n",
    "\n",
    "    if depth == 0:\n",
    "        return x * p_acc + u_bad_1 * (1 - p_acc)\n",
    "\n",
    "    # Accept branch uses original p\n",
    "    continuation_accept = (1 - p) * expected_utility_p1_dynamic(x, alpha, depth - 1, u_bad_1, u_resp, p, p_bump, u_bad_2) + p * u_resp\n",
    "\n",
    "    # Reject branch uses bumped p\n",
    "    bumped_p = min(p + p_bump, 1.0)\n",
    "    continuation_reject = (1 - bumped_p) * expected_utility_p1_dynamic(x, alpha, depth - 1, u_bad_1, u_resp, p, p_bump, u_bad_2) + bumped_p * u_resp\n",
    "\n",
    "    utility_if_accepted = x + continuation_accept\n",
    "    utility_if_rejected = u_bad_1 + continuation_reject\n",
    "\n",
    "    return p_acc * utility_if_accepted + (1 - p_acc) * utility_if_rejected\n",
    "\n",
    "def neg_expected_utility_p1_dynamic(x):\n",
    "    return -expected_utility_p1_dynamic(x, _alpha, _depth, _u_bad_1, _u_resp, _p, _p_bump, _u_bad_2)\n",
    "\n",
    "def optimize_x_dynamic(alpha, depth, u_bad_1, u_resp, p, p_bump, u_bad_2):\n",
    "    global _alpha, _depth, _u_bad_1, _u_resp, _p, _p_bump, _u_bad_2\n",
    "    _alpha = alpha\n",
    "    _depth = depth\n",
    "    _u_bad_1 = u_bad_1\n",
    "    _u_resp = u_resp\n",
    "    _p = p\n",
    "    _p_bump = p_bump\n",
    "    _u_bad_2 = u_bad_2\n",
    "\n",
    "    result = minimize_scalar(neg_expected_utility_p1_dynamic, bounds=(0, 100), method='bounded')\n",
    "    return result.x, -result.fun\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5164cc",
   "metadata": {},
   "source": [
    "### If the players are not aware of each other's strategies\n",
    "\n",
    "Then we quickly reach an equilibrium: without awareness of the proposer's strategy, the responder wants to get as close to utilitarian as they can. They are not aware that if they were more willing to take risks and reject some deals then the proposer will update strategy to protect itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87447752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(50.0, 0.1),\n",
       " (50.530503530698184, 0.1),\n",
       " (50.530503530698184, 9.999994793574977),\n",
       " (74.26923306063837, 9.999994793574977),\n",
       " (74.26923306063837, 9.999993236301638),\n",
       " (74.26923296253761, 9.999993236301638),\n",
       " (74.26923296253761, 9.999993236299266),\n",
       " (74.26923296253777, 9.999993236299266),\n",
       " (74.26923296253777, 9.999993236299266),\n",
       " (74.26923296253777, 9.999993236299266),\n",
       " (74.26923296253777, 9.999993236299266),\n",
       " (74.26923296253777, 9.999993236299266),\n",
       " (74.26923296253777, 9.999993236299266),\n",
       " (74.26923296253777, 9.999993236299266),\n",
       " (74.26923296253777, 9.999993236299266),\n",
       " (74.26923296253777, 9.999993236299266),\n",
       " (74.26923296253777, 9.999993236299266),\n",
       " (74.26923296253777, 9.999993236299266),\n",
       " (74.26923296253777, 9.999993236299266),\n",
       " (74.26923296253777, 9.999993236299266),\n",
       " (74.26923296253777, 9.999993236299266)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulation constants\n",
    "depth = 3          # recursion depth (4 rounds total)\n",
    "u_bad_1 = -75      # Player 1's bad outcome\n",
    "u_bad_2 = -75      # Player 2's bad outcome\n",
    "u_resp = -25        # Player 1's utility when not proposer\n",
    "p = 0.25            # probability of control switching\n",
    "n_steps = 20       # number of updates in trajectory\n",
    "\n",
    "# Initial strategies\n",
    "x = 50.0\n",
    "alpha = 0.1\n",
    "trajectory = [(x, alpha)]\n",
    "\n",
    "n_steps = 20  # or whatever number of steps you want\n",
    "for step in range(n_steps):\n",
    "    if step % 2 == 0:\n",
    "        x_val, _ = optimize_x(alpha, depth, u_bad_1, u_resp, p, u_bad_2)\n",
    "        x = x_val\n",
    "    else:\n",
    "        alpha_val = optimize_alpha(x, u_bad_2)\n",
    "        alpha = alpha_val\n",
    "\n",
    "    trajectory.append((x, alpha))\n",
    "\n",
    "trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d672f2",
   "metadata": {},
   "source": [
    "### Now Add Rejection Flip Bump\n",
    "\n",
    "This too makes no difference. As long as the responder doesn't know that that the proposer will respond to its strategies, the proposer has the advantage here, as the responder is short-term risk averse since they lack the ability to consider the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a9370b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(50.0, 0.1),\n",
       " (49.931342707538995, 0.1),\n",
       " (49.931342707538995, 9.999994793574977),\n",
       " (74.26094295543456, 9.999994793574977),\n",
       " (74.26094295543456, 9.99999303871964),\n",
       " (74.26094284317034, 9.99999303871964),\n",
       " (74.26094284317034, 9.999993038717003),\n",
       " (74.26094284275251, 9.999993038717003),\n",
       " (74.26094284275251, 9.99999303871699),\n",
       " (74.26094284275251, 9.99999303871699),\n",
       " (74.26094284275251, 9.99999303871699),\n",
       " (74.26094284275251, 9.99999303871699),\n",
       " (74.26094284275251, 9.99999303871699),\n",
       " (74.26094284275251, 9.99999303871699),\n",
       " (74.26094284275251, 9.99999303871699),\n",
       " (74.26094284275251, 9.99999303871699),\n",
       " (74.26094284275251, 9.99999303871699),\n",
       " (74.26094284275251, 9.99999303871699),\n",
       " (74.26094284275251, 9.99999303871699),\n",
       " (74.26094284275251, 9.99999303871699),\n",
       " (74.26094284275251, 9.99999303871699)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add new constant\n",
    "p_bump = 0.1  # how much to increase p after a rejection\n",
    "\n",
    "# Updated alternating update loop using the dynamic expected utility\n",
    "x = 50.0\n",
    "alpha = 0.1\n",
    "trajectory = [(x, alpha)]\n",
    "\n",
    "for step in range(n_steps):\n",
    "    if step % 2 == 0:\n",
    "        # Player 1 updates x using dynamic utility with p_bump\n",
    "        x_val, _ = optimize_x_dynamic(alpha, depth, u_bad_1, u_resp, p, p_bump, u_bad_2)\n",
    "        x = x_val\n",
    "    else:\n",
    "        # Player 2 updates alpha as before\n",
    "        alpha_val = optimize_alpha(x, u_bad_2)\n",
    "        alpha = alpha_val\n",
    "\n",
    "    trajectory.append((x, alpha))\n",
    "\n",
    "trajectory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0cb1c5",
   "metadata": {},
   "source": [
    "Based on this realization, responder also needs to have recursive long-term utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cb2f986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_utility_p2_recursive(x, alpha, depth, u_bad_2):\n",
    "    p_acc = p_accept(x, alpha, u_bad_2)\n",
    "\n",
    "    if depth == 0:\n",
    "        return -x * p_acc + u_bad_2 * (1 - p_acc)\n",
    "\n",
    "    continuation = expected_utility_p2_recursive(x, alpha, depth - 1, u_bad_2)\n",
    "\n",
    "    utility_if_accepted = -x + continuation\n",
    "    utility_if_rejected = u_bad_2 + continuation\n",
    "\n",
    "    return p_acc * utility_if_accepted + (1 - p_acc) * utility_if_rejected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3261fee5",
   "metadata": {},
   "source": [
    "### Player 2 chooses alpha, assuming Player 1 best-responds to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "508d13ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_alpha_level_k(depth, u_bad_1, u_resp, p, p_bump, u_bad_2):\n",
    "    def neg_recursive_utility(alpha):\n",
    "        x_star, _ = optimize_x_dynamic(alpha, depth, u_bad_1, u_resp, p, p_bump, u_bad_2)\n",
    "        return -expected_utility_p2_recursive(x_star, alpha, depth, u_bad_2)\n",
    "\n",
    "    result = minimize_scalar(neg_recursive_utility, bounds=(0.01, 10), method='bounded')\n",
    "    return result.x \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b650a489",
   "metadata": {},
   "source": [
    "### Now re-run simulation with player 2 aware of player 1's response\n",
    "\n",
    "Suddenly, player 1 wants to be as high variance as possible to make player 1 give lower offers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79e15ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(50.0, 0.1),\n",
       " (49.931342707538995, 0.1),\n",
       " (49.931342707538995, 0.05046804023833471),\n",
       " (42.400440611120096, 0.05046804023833471),\n",
       " (42.400440611120096, 0.05046804023833471),\n",
       " (42.400440611120096, 0.05046804023833471),\n",
       " (42.400440611120096, 0.05046804023833471),\n",
       " (42.400440611120096, 0.05046804023833471),\n",
       " (42.400440611120096, 0.05046804023833471),\n",
       " (42.400440611120096, 0.05046804023833471),\n",
       " (42.400440611120096, 0.05046804023833471),\n",
       " (42.400440611120096, 0.05046804023833471),\n",
       " (42.400440611120096, 0.05046804023833471),\n",
       " (42.400440611120096, 0.05046804023833471),\n",
       " (42.400440611120096, 0.05046804023833471),\n",
       " (42.400440611120096, 0.05046804023833471),\n",
       " (42.400440611120096, 0.05046804023833471),\n",
       " (42.400440611120096, 0.05046804023833471),\n",
       " (42.400440611120096, 0.05046804023833471),\n",
       " (42.400440611120096, 0.05046804023833471),\n",
       " (42.400440611120096, 0.05046804023833471)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 50.0\n",
    "alpha = 0.1\n",
    "trajectory = [(x, alpha)]\n",
    "\n",
    "for step in range(n_steps):\n",
    "    if step % 2 == 0:\n",
    "        x_val, _ = optimize_x_dynamic(alpha, depth, u_bad_1, u_resp, p, p_bump, u_bad_2)\n",
    "        x = x_val\n",
    "    else:\n",
    "        alpha_val = optimize_alpha_level_k(depth, u_bad_1, u_resp, p, p_bump, u_bad_2)\n",
    "        alpha = alpha_val\n",
    "\n",
    "    trajectory.append((x, alpha))\n",
    "\n",
    "trajectory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d323575b",
   "metadata": {},
   "source": [
    "### But this is perhaps unfair to player 1: if player 1 knew player 2 knew how player 1 would respond..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ea2c94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a fixed x, return the best alpha assuming Player 2 is forward-looking\n",
    "def optimize_alpha_level_k_given_x(x, depth, u_bad_2):\n",
    "    def neg_recursive_utility(alpha):\n",
    "        return -expected_utility_p2_recursive(x, alpha, depth, u_bad_2)\n",
    "    \n",
    "    result = minimize_scalar(neg_recursive_utility, bounds=(0.01, 10), method='bounded')\n",
    "    return result.x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4ed379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Level-k Player 1 expected utility, anticipating that Player 2 will respond with alpha*(x)\n",
    "def player1_levelk_objective(x, depth, u_bad_1, u_resp, p, p_bump, u_bad_2):\n",
    "    alpha_star = optimize_alpha_level_k_given_x(x, depth, u_bad_2)\n",
    "    return -expected_utility_p1_dynamic(x, alpha_star, depth, u_bad_1, u_resp, p, p_bump, u_bad_2)\n",
    "\n",
    "# Optimizer for level-k Player 1\n",
    "def optimize_x_level_k(depth, u_bad_1, u_resp, p, p_bump, u_bad_2):\n",
    "    def objective(x):\n",
    "        return player1_levelk_objective(x, depth, u_bad_1, u_resp, p, p_bump, u_bad_2)\n",
    "    \n",
    "    result = minimize_scalar(objective, bounds=(0, 100), method='bounded')\n",
    "    return result.x, -result.fun\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e8f284",
   "metadata": {},
   "source": [
    "### So now we have both players as level-k thinkers\n",
    "\n",
    "We have a fascinating (and bad) result: The equilibrium here is for the proposer to take a strong line, offering just below the decision line, and for the proposer to take an incredibly high variance approach. This result doesn't really make sense to me at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c4ef0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(50.0, 0.1),\n",
       " (74.26094297539323, 0.1),\n",
       " (74.26094297539323, 0.05046804023833471),\n",
       " (74.26094297539323, 0.05046804023833471),\n",
       " (74.26094297539323, 0.05046804023833471),\n",
       " (74.26094297539323, 0.05046804023833471),\n",
       " (74.26094297539323, 0.05046804023833471),\n",
       " (74.26094297539323, 0.05046804023833471),\n",
       " (74.26094297539323, 0.05046804023833471),\n",
       " (74.26094297539323, 0.05046804023833471),\n",
       " (74.26094297539323, 0.05046804023833471),\n",
       " (74.26094297539323, 0.05046804023833471),\n",
       " (74.26094297539323, 0.05046804023833471),\n",
       " (74.26094297539323, 0.05046804023833471),\n",
       " (74.26094297539323, 0.05046804023833471),\n",
       " (74.26094297539323, 0.05046804023833471),\n",
       " (74.26094297539323, 0.05046804023833471),\n",
       " (74.26094297539323, 0.05046804023833471),\n",
       " (74.26094297539323, 0.05046804023833471),\n",
       " (74.26094297539323, 0.05046804023833471),\n",
       " (74.26094297539323, 0.05046804023833471)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 50.0\n",
    "alpha = 0.1\n",
    "trajectory = [(x, alpha)]\n",
    "\n",
    "for step in range(n_steps):\n",
    "    if step % 2 == 0:\n",
    "        # Player 1 chooses x knowing Player 2 will respond with alpha*(x)\n",
    "        x_val, _ = optimize_x_level_k(depth, u_bad_1, u_resp, p, p_bump, u_bad_2)\n",
    "        x = x_val\n",
    "    else:\n",
    "        # Player 2 chooses alpha knowing Player 1 will respond with x*(alpha)\n",
    "        alpha_val = optimize_alpha_level_k(depth, u_bad_1, u_resp, p, p_bump, u_bad_2)\n",
    "        alpha = alpha_val\n",
    "\n",
    "    trajectory.append((x, alpha))\n",
    "\n",
    "trajectory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa48c852",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
